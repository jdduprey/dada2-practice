library(dada2); packageVersion('dada2')
# THIS PIPELINE BEGINS WITH DEMULTIPLEXED FASTQ FILES
# so prior to this step you need to run Ramon's demultiplexer for dada2
#--------------------------------------------------------------------------
#demultiplexer_for_DADA2
#If you want to use DADA2 with libraries created by adding adapters through ligation and with two levels of adapters -
#see banzai (github.com/jimmyodonnell/banzai) - you need to demultiplex samples before pairing both ends.
#This script will look for your adapters and pcr primers on your reads,
#and return 4 fastq files per unique sample: Fwd.1, Fwd.2, Rev.1 and Rev.2.
#---------------------------------------------------------------------------
#BEGIN dada2 tutorial
#---------------------------------------------------------------------------
# path to the Miseq data files
path <- "./MiSeq_SOP"
list.files(path)
# Forward and reverse fastq filenames have format: SAMPLENAME_R1_001.fastq and SAMPLENAME_R2_001.fastq
fnFs <- sort(list.files(path, pattern="_R1_001.fastq", full.names = TRUE))
fnRs <- sort(list.files(path, pattern="_R2_001.fastq", full.names = TRUE))
# Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq
sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)
plotQualityProfile(fnFs[1:2])
plotQualityProfile(fnRs[1:2])
#In gray-scale is a heat map of the frequency of each quality score at each base position.
#The mean quality score at each position is shown by the green line'''
# Place filtered files in filtered subdirectory
filtFs <- file.path(path, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(path, "filtered", paste0(sample.names, "_R_filt.fastq.gz"))
names(filtFs) <- sample.names
names(filtRs) <- sample.names
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(240,160),
maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
compress=TRUE, multithread=TRUE)
head(out)
#learn the error rates
errF <- learnErrors(filtFs, multithread=TRUE)
errR <- learnErrors(filtRs, multithread=TRUE)
plotErrors(errF, nominalQ=TRUE)
plotErrors(errR, nominalQ=TRUE)
#sample inference
dadaFs <- dada(filtFs, err=errF, multithread=TRUE)
dadaRs <- dada(filtRs, err=errR, multithread=TRUE)
# Inspecting the returned dada-class object:
dadaFs[[1]]
# about dada class
help("dada-class")
# merge paired reads
#--------------------------------------------------------------
# "merge the forward and reverse reads together to obtain the full denoised sequences.
# Merging is performed by aligning the denoised forward reads with the reverse-complement
# of the corresponding denoised reverse reads, and then constructing
# the merged “contig” sequences. By default, merged sequences are only output
# if the forward and reverse reads overlap by at least 12 bases" -dada2 tutorial
mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE)
# Inspect the merger data.frame from the first sample
head(mergers[[1]])
# Construct sequence table
#--------------------------------------------------------------
# "We can now construct an amplicon sequence variant table (ASV) table"
seqtab <- makeSequenceTable(mergers)
dim(seqtab)
# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))
# Remove chimeras
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim)
sum(seqtab.nochim)/sum(seqtab)
# Track reads through the pipeline
#As a final check of our progress, we’ll look at the number of reads that made it
#through each step in the pipeline:
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))
# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
head(track)
# Assign taxonomy
# ---------------------------------------------------------------
# "It is common at this point, especially in 16S/18S/ITS amplicon sequencing,
# to assign taxonomy to the sequence variants. The DADA2 package provides a
# native implementation of the naive Bayesian classifier method for this purpose.
# The assignTaxonomy function takes as input a set of sequences to be classified
# and a training set of reference sequences with known taxonomy, and outputs taxonomic
# assignments with at least minBoot bootstrap confidence." -dada2 tutorial
taxa <- assignTaxonomy(seqtab.nochim, "./taxa/silva_nr_v132_train_set.fa.gz", multithread=TRUE)
taxa.print <- taxa # Removing sequence rownames for display only
rownames(taxa.print) <- NULL
head(taxa.print)
unqs.mock <- seqtab.nochim["Mock",]
unqs.mock <- sort(unqs.mock[unqs.mock>0], decreasing=TRUE) # Drop ASVs absent in the Mock
cat("DADA2 inferred", length(unqs.mock), "sample sequences present in the Mock community.\n")
View(seqtab.nochim)
unqs.mock <- seqtab.nochim["Mock",]
unqs.mock <- sort(unqs.mock[unqs.mock>0], decreasing=TRUE) # Drop ASVs absent in the Mock
cat("DADA2 inferred", length(unqs.mock), "sample sequences present in the Mock community.\n")
mock.ref <- getSequences(file.path(path, "HMP_MOCK.v35.fasta"))
mock.ref <- getSequences(file.path(path, "HMP_MOCK.v35.fasta"))
mock.ref <- getSequences(file.path(path, "HMP_MOCK.v35.fasta"))
mock.ref <- getSequences(file.path(path, "HMP_MOCK.v35.fasta"))
match.ref <- sum(sapply(names(unqs.mock), function(x) any(grepl(x, mock.ref))))
cat("Of those,", sum(match.ref), "were exact matches to the expected reference sequences.\n")
